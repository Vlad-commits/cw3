\documentclass[%
bachelor,    % тип документа
%natbib,      % использовать пакет natbib для "сжатия" цитирований
subf,        % использовать пакет subcaption для вложенной нумерации рисунков
href,        % использовать пакет hyperref для создания гиперссылок
colorlinks,  % цветные гиперссылки
%fixint,     % включить прямые знаки интегралов
]{disser}

\usepackage[
  a4paper, mag=1000,
  left=2.5cm, right=1cm, top=2cm, bottom=2cm, headsep=0.7cm, footskip=1cm
]{geometry}


\usepackage[intlimits]{amsmath}
\usepackage{amssymb,amsfonts}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{pgf}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi
\usepackage[autostyle]{csquotes}

% Шрифт Times в тексте как основной
%\usepackage{tempora}
% альтернативный пакет из дистрибутива TeX Live
%\usepackage{cyrtimes}

% Шрифт Times в формулах как основной
%\usepackage[varg,cmbraces,cmintegrals]{newtxmath}
% альтернативный пакет
%\usepackage[subscriptcorrection,nofontinfo]{mtpro2}

\usepackage[%
  style=gost-numeric,
  backend=biber,
  language=auto,
  hyperref=auto,
  autolang=other,
  sorting=none
]{biblatex}

\addbibresource{report.bib}

% Плавающие рисунки "в оборку".
\usepackage{wrapfig}

% Номера страниц снизу и по центру
%\pagestyle{footcenter}
%\chapterpagestyle{footcenter}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% plots fixes
%\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{pgfplots}
%\usepackage{tikz}
%\usepgfplotslibrary{external} 
%\usetikzlibrary{external}
%\tikzexternalize


% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{afterpage}
\usepackage{amsthm}
%\usepackage[T1]{fontenc}
%\usepackage{cmbright}
\usepackage{hyperref}
\hypersetup{
	colorlinks=false,% make the links colored
}
\begin{document}

% Переопределение стандартных заголовков
%\def\contentsname{Содержание}
%\def\conclusionname{Выводы}
%\def\bibname{Литература}

%
% Титульный лист на русском языке
%

\institution{
	Санкт-Петербургский государственный университет\\
	Математико-Механический факультет\\
	Кафедра прикладной кибернетики\\
}


\title{Отчёт по научно-исследовательской работе}
\def\topiclabel{}
\topic{Сравнительный анализ систем загрузки больших данных}

% Автор
\author{Мамаев Владислав Викторович}
% Группа
\group{323}
% Номер специальности
\coursenum{010400 (01.03.02)}
% Название специальности
\course{Прикладная математика и информатика}

% Научный руководитель

\sa      {Благов Михаил Валерьевич}

\sastatus{к.~ф.-м.~н.}
% Город и год
\city{Санкт-Петербург}
\date{\number\year}

\maketitle

%%
%% Titlepage in English
%%
%
%\institution{Name of Organization}
%
%% Approved by
%\apname{Professor S.\,S.~Sidorov}
%
%\title{Bachelor's Thesis}
%
%% Topic
%\topic{Dummy Title}
%
%% Author
%\author{Author's Name} % Full Name
%\course{Physics} % Specialization
%
%\group{} % Study Group
%
%% Scientific Advisor
%\sa       {I.\,I.~Ivanov}
%\sastatus {Professor}
%
%% Reviewer
%\rev      {P.\,P.~Petrov}
%\revstatus{Associate Professor}
%
%% Consultant
%\con{}
%\conspec{}
%\constatus{}
%
%% City & Year
%\city{Saint Petersburg}
%\date{\number\year}
%
%\maketitle[en]

% Содержание

\begin{center}
	\textbf{Отзыв научного руководителя}
\end{center}

Тема работы является актуальной. При выполнении работы студент проявил самостоятельность. Работа выполнена на хорошем уровне, отчёт составлен грамотно и достаточно полно отражает результаты работы студента.


Научно-исследовательская работа выполнена полностью и может быть зачтена.

В ходе работы студент приобрел следующие компетенции: ПКП-1,ПКП2, ПКП-4, УКБ-1, УКБ-2, УКБ-7.

Оценка A
\begin{flushright}
к.~ф.-м.~н. Благов Михаил Валерьевич
\end{flushright}

\tableofcontents


\section{Введение}
%Что такое OLTP и OLAP? Для чего нужны?
  
Базы данных используются повсеместно, при этом системы, для которых они предназначены, можно разделить на 2 класса:\cite{oltp_olap_2}
\begin{itemize}
\item Online Transaction Processing (OLTP) -- системы обработки транзакций в реальном времени. Такие системы используются для операционной деятельности предприятий.
\item Online Analytical Processing (OLAP) -- системы аналитической обработки в реальном времени. К такими системам относятся системы поддержки принятия решений, инструменты business intelligence, системы анализа данных.
\end{itemize}

% Почему их следует разделять? 
OLAP системы содержат информацию из OTLP систем, при обычно OLAP системы обычно функционирует независимо от OLTP систем по следующим причинам:\cite{oltp_olap}
\begin{itemize}
	\item Реляционная схема данных, обычно используемая в OLTP системах, не эффективна для OLAP нагрузки.
	\item OLAP нагрузка на OLTP систему может вызвать проблемы с производительностью транзакций.
	\item С увеличением размера хранимых данных, возникают ограничения на используемые технологии и существенно увеличивается стоимость хранения данных в системах не предназначенных только для OLAP.
	\item Необходимость доступа к данным из разных OLAP систем и возможности ограничения доступа к данным
\end{itemize}

% Какие проблемы при интеграции?
Традиционно, данные попадают в OLAP систему из OLTP системы при помощи Extraction-Transformation-Loading (ETL) программ,
запускаемых с некоторой периодичностью. ETL процессы могут занимать достаточно много времени, и это создает задержку появления данных в OLAP системе. 

Для того чтобы уменьшить задержку можно вместо периодичных ETL процессов использовать потоковую обработку.
Подобная архитектура интеграции данных описана в \cite{streaming_integration} \cite{streaming_integration_2}.
%  добавить диаграмму?
Каждое изменение данных в OLTP системе записывается в очередь сообщений, а некоторая программа непрерывно читает сообщения и обновляет данные в аналитическом хранилище.
Использование непрерывных обновлений уменьшает задержку, но требует специальных инструментов для обеспечения целостности данных --- традиционно используемые хранилища для большого объема данных, такие как Hadoop, работают по принципу write-once и не поддерживают обновлений.

\subsection{Постановка задачи}
В данной работе будет проведен сравнительный анализ систем для создания ETL процессов загрузки больших данных: Apache Hudi, Delta Lake, Apache Iceberg.
Сравнение будет произведено по следующим пунктам:
\begin{itemize}
	\item Качественные критерии: удобство доступа к данным --- интеграция с другими инструментами для работы с большими данными и совместимость, возможность использования различных хранилищ.

	\item Производительность: пропускная способность и время задержки доставки данных 

\end{itemize}
\newpage
\section{Основная часть}
\subsection{Поддержка различных хранилищ}
%https://hudi.apache.org/docs/overview.html
%https://docs.delta.io/latest/delta-storage.html
%https://iceberg.apache.org/reliability/
Для хранения данных аналитических данных необходима технология для хранения файлов. Традиционно в качестве такой технологии используется HDFS. HDFS  --- Hadoop Distributed File System, распределённая файловая система, часть проекта Hadoop. Альтернативами для HDFS явлются облачные файловые системы, такие как Amazon S3, Microsoft Azure Storage, Google Cloud Storage.

Сравниваемые инструменты поддерживают использование HDFS в качестве хранилища данных. Помимо этого Hudi поддерживает совместимые с Hadoop хранилища, включая Amazon S3, Microsoft Azure Storage, Google Cloud Storage, IBM Cloud Object Storage, Alibaba Cloud OSS, Tencent Cloud Object Storage.

Iceberg поддерживает любые хранилища для хранения данных, однако для храненения метаданных необходимо хранилище, поддерживающее атомарное переименование файлов.

Для корректной работы Delta Lake файловое хранилище должно обеспечивать: атомарную видимость файлов, атомарное создание и переименование файлов, согласованную операцию чтения списка файлов --- после записи все последующие чтения списка должны возвращать записанный файл. Встроенная поддержка обеспечена для Amazon S3 (без гарантии отсутствия потерь данных в случае конкурентной записи), Microsoft Azure storage, Google Cloud Storage, IBM Cloud Object Storage, Oracle Cloud Infrastructure.

\subsection{Поддержка Spark}
Наиболее распространённым инструментом для ETL процессов является Apache Spark. Apache Spark - это система для обработки больших данных.\cite{spark}
Delta Lake версии 0.7.0 и выше поддерживает работу со Spark 3.x.x. Hudi поддерживает 2 и 3 версии Spark. Iceberg предоставляет дистрибутивы для работы со Spark 2 и 3. Однако, при использовании Spark 2, некоторые функии недоступны. Одна из таких функций --- это SQL merge into, операция, позволяющая обновить таблицу используя новые данные, добавив строку либо обновив существующую, в зависимости от того выполнено ли условие. Ввиду отсутствия поддержки данной функции, Iceberg будет исключён из сравнения производительности.

%https://iceberg.apache.org/spark-writes/
\subsection{Интеграция с другими инструментами}
Hive --- это система, которая упрощает чтение, запись и управление большими наборами данных, находящимися в распределенном хранилище, с помощью Hive Query Language. Может работать с данными , уже находящимися в хранилище. Для подключения пользователей к Hive предоставляются инструмент командной строки и драйвер JDBC.\cite{hive} Все инструменты поддерживают работу с Apache Hive 2.x.


Apache Flink - это инструмент распределённой обработки для вычислений с отслеживанием состояния в неограниченных и ограниченных потоках данных.\cite{flink}
Hudi работает с Flink 1.12.x. Iceberg работает с Apache Flink 1.11.x. Delta Lake не поддерживает Flink



%\subsection{Безопасность}
%Для Delta Lake существует Unity Catalog lets you mount existing data in Apache Hive Metastores or Amazon S3 and manage policies across advanced security solutions like Immuta or Privacera, while using ANSI SQL DCL to manage permissions, all in one place. https://databricks.com/product/unity-catalog

\subsection{Производительность}
В качестве тестовых данных использовались сгенерированные записи событий с датчиков. Такие данные были выбраны так, как часто возникают в задаче интеграции OLTP и OLAP систем для интернета вещей.\cite{test_data}\cite{iot_case1}\cite{iot_case2} В качестве входных данных используется такой набор данных в котором каждая запись уже содержится в таблице.

На рисунках \ref{fig:1},\ref{fig:2},\ref{fig:3} приведены зависимости времени работы ETL программ с использованием различных технологий от размера входных данных при различных конфигурациях ресурсов кластера Spark и различном количестве записей в таблице.

\begin{figure}
	\input{fig/1.pgf}
	\caption{Зависимост времени работы от количества записей, 1 executor, 12 ядер и 10Gb памяти, $10^6$ записей в таблице}
	\label{fig:1}
\end{figure}
\begin{figure}
	\input{fig/2.pgf}
	\caption{Зависимост времени работы от количества записей, 2 executors, 3 ядра и 6Gb памяти каждый, $10^7$ записей в таблице}
	\label{fig:2}
\end{figure}
\begin{figure}
	\input{fig/3.pgf}
	\caption{Зависимост времени работы от количества записей, 4 executors, 3 ядра и 6Gb памяти каждый, $10^7$ записей в таблице}
	\label{fig:3}
\end{figure}
\newpage
\section{Заключение}
В рамках работы были описаны качественные различия трёх систем загрузки больших данных и проведено сравнение производительности.
% Список литературы
\printbibliography[heading=bibintoc]

% Приложения
\appendix


\end{document}